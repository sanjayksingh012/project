{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "316c16b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Using 20000 files for training.\n",
      "['neg', 'pos']\n",
      "Found 25000 files belonging to 2 classes.\n",
      "Using 5000 files for validation.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "############################# LOAD DATA\n",
    "############################# LOad data, tf.data\n",
    "import tensorflow as tf\n",
    "train_dir='C:\\\\Users\\\\SANJAY\\\\SENTIMENT_ANALYSIS\\\\aclImdb\\\\train'\n",
    "test_dir='C:\\\\Users\\\\SANJAY\\\\SENTIMENT_ANALYSIS\\\\aclImdb\\\\test'\n",
    "batch_size=32\n",
    "seed=123\n",
    "raw_train_ds=tf.keras.preprocessing.text_dataset_from_directory(train_dir,batch_size=batch_size,\n",
    "        seed=seed,validation_split=0.2,subset='training')\n",
    "print(raw_train_ds.class_names)\n",
    "raw_val_ds=tf.keras.preprocessing.text_dataset_from_directory(train_dir,batch_size=batch_size,\n",
    "        seed=seed,validation_split=0.2,subset='validation')\n",
    "raw_test_ds=tf.keras.preprocessing.text_dataset_from_directory(test_dir,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d22e3f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review:- b'After, I watched the films... I thought, \"Why the heck was this film such a high success in the Korean Box Office?\" Even thought the movie had a clever/unusal scenario, the acting wasn\\'t that good and the characters weren\\'t very interesting. For a Korean movie... I liked the fighting scenes. If you want to watch a film without thinking, this is the film for you. But I got to admit... the film was kind of childish... 6/10'\n",
      "Label:- 1\n",
      "Review:- b'Or released on DVD or screened on a cable channel like Amer. Life TV network. I have been watching another favorite, \"Voyage to the Bottom of the Sea\", as well as \"Lost in Space\" and Land of Giants\". They\\'ve been showing them forever but aren\\'t receptive to suggestions for other shows. My father and I were big fans as I was already a big science/electronics nut, (still am) and my father was an old school chum of Nader. They both attended Oxy together. I still have memories of several of the episodes even though I was only 9. More so than any show that old. I think it was televised on Sat. after \"Bonanza\". Some of the episodes I recall are the one where he takes the experimental drug that slows down action. Or the one where he body surfs the big ones, (I did that too!) Or the one where there was a mine cave in and he conveys how to use mind control to have the trapped people slow their breathing by entering a trance-like state. That is the one show that I wish I could see again. I got my wish with the original \"Outer Limits\" and \"Sci-Fi Theater...John'\n",
      "Label:- 1\n",
      "Review:- b\"I saw Insomniac's Nightmare not to long ago for the first time and I have to say, I really found it to be quite good. If you are a fan of Dominic Monaghan you will love it. The hole movie takes place inside his mind -or does it? The acting from everyone else is a little rushed and shaky and some of the scenes could be cut down but it works out in the end. The extras on the DVD are just as great as the film, if not greater for those Dom fans. It has tons of candid moments from the set, outtakes and a great interview with the director. Anyone who has gone through making an independent film will love to watch Tess (the director), Dom and everyone else on the very small close personal set try to bang out this little trippy creepy film. It was pretty enjoyable and I'm glad to have it in my collection.\"\n",
      "Label:- 1\n"
     ]
    }
   ],
   "source": [
    "for text_batch,label_batch in raw_train_ds.take(1):\n",
    "    for i in range(3):\n",
    "        print('Review:-',text_batch[i].numpy())\n",
    "        print('Label:-',label_batch[i].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "380e8743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lebel 1 pos\n",
      "Lebel 0 neg\n"
     ]
    }
   ],
   "source": [
    "print('Lebel 1',raw_train_ds.class_names[1])\n",
    "print('Lebel 0',raw_train_ds.class_names[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "256838af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "print(string.punctuation)\n",
    "#......."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1192fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################preprocessing \n",
    "# s->A|b,cl\n",
    "import re\n",
    "def cust_stand(input_data):\n",
    "    lower=tf.strings.lower(input_data)\n",
    "    html=tf.strings.regex_replace(lower,'<br />','')\n",
    "    punc=tf.strings.regex_replace(html,'[%s]'%re.escape(string.punctuation),'')\n",
    "    return punc\n",
    "max_features=10000 # dictionary size, number od words\n",
    "seq_length=250 # fixed lenth\n",
    "vectorize_layer=tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
    "    standardize=cust_stand,max_tokens=max_features,\n",
    "                                 output_mode='int',output_sequence_length=seq_length)\n",
    "train_text=raw_train_ds.map(lambda x,y:x)\n",
    "vectorize_layer.adapt(train_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "edb0504f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(text,label):\n",
    "    text=tf.expand_dims(text,-1) # batch\n",
    "    return vectorize_layer(text),label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2db5be7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review- tf.Tensor(b'Poor Tobe Hopper. He directed an all time horror classic \"Texas Chaimsaw Massacre\". Since then everything he\\'s done has been horrible. This is probably the worst...and that\\'s saying a lot. It\\'s about a man (Brad Dourif) who has the ability to make things (and people) catch fire...or something like that. Hardly an original idea (anyone remember \"Firestarter\"?) It\\'s a real mess...literally EVERYTHING is done wrong! Pathetic acting (even Dourif!), asinine script, loust production values, crappy special effects...everything is BAD!!!!! A must miss...not even good for laughs.', shape=(), dtype=string)\n",
      "Label tf.Tensor(0, shape=(), dtype=int32)\n",
      "Vectorize text (<tf.Tensor: shape=(1, 250), dtype=int64, numpy=\n",
      "array([[ 339, 7903, 4642,   27,  531,   33,   31,   61,  196,  341, 2379,\n",
      "           1, 3240,  233,   93,  288,  225,  219,   43,   73,  524,   10,\n",
      "           7,  235,    2,    1,  178,  650,    4,  168,   29,   42,    4,\n",
      "         130, 2853,    1,   35,   43,    2, 1220,    6,   92,  181,    3,\n",
      "          77, 1193,    1,  139,   37,   12,  949,   33,  195,  314,  250,\n",
      "         366,    1,   29,    4,  144,    1,  288,    7,  219,  354, 1197,\n",
      "         113,   55,    1,    1,  230,    1,  368, 1206, 1981,  307,    1,\n",
      "           7,   79,    4,  217,    1,   55,   49,   16,  945,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0]], dtype=int64)>, <tf.Tensor: shape=(), dtype=int32, numpy=0>)\n"
     ]
    }
   ],
   "source": [
    "text_batch,label_batch=next(iter(raw_train_ds))\n",
    "r1,l1=text_batch[0],label_batch[0]\n",
    "print('Review-',r1)\n",
    "print('Label',l1)\n",
    "print('Vectorize text',vectorize_text(r1,l1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "870be021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "back\n",
      "causes\n"
     ]
    }
   ],
   "source": [
    "print(vectorize_layer.get_vocabulary()[141])\n",
    "print(vectorize_layer.get_vocabulary()[2870])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd6748f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds=raw_train_ds.map(vectorize_text)\n",
    "test_ds=raw_test_ds.map(vectorize_text)\n",
    "val_ds=raw_val_ds.map(vectorize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b7298e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 16)          160016    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, None, 16)          0         \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 16)               0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 160,033\n",
      "Trainable params: 160,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "###################create model\n",
    "model=tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(10001,16)) # number of feature+1,embedding size\n",
    "model.add(tf.keras.layers.Dropout(0.2)) # 20% random connection close\n",
    "model.add(tf.keras.layers.GlobalAveragePooling1D()) # raplace a vector by aberage\n",
    "model.add(tf.keras.layers.Dense(1))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e075434e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "625/625 [==============================] - 59s 92ms/step - loss: 0.8332 - accuracy: 0.5278 - val_loss: 0.6789 - val_accuracy: 0.5696\n",
      "Epoch 2/2\n",
      "625/625 [==============================] - 13s 21ms/step - loss: 0.6318 - accuracy: 0.7122 - val_loss: 0.5914 - val_accuracy: 0.8184\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "hist=model.fit(train_ds,validation_data=val_ds,epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1e21565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 230s 290ms/step - loss: 0.5978 - accuracy: 0.8000\n",
      "0.7999600172042847\n"
     ]
    }
   ],
   "source": [
    "test_loss,test_acc=model.evaluate(test_ds)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0f73061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n",
      "Training acc [0.5277500152587891, 0.7122499942779541]\n"
     ]
    }
   ],
   "source": [
    "model_dict=hist.history\n",
    "print(model_dict.keys())\n",
    "print('Training acc',model_dict['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea007164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 11s 13ms/step - loss: 0.6951 - accuracy: 0.5000\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "export_model=tf.keras.Sequential([\n",
    "    vectorize_layer,model,tf.keras.layers.Activation('sigmoid')\n",
    "])\n",
    "export_model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "test_loss,test_acc=export_model.evaluate(raw_test_ds)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc0f42b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I like this movie.\n",
      "tf.Tensor([b'I like this movie.'], shape=(1,), dtype=string)\n",
      "[[0.63102347]]\n",
      "[['POS']]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "f=open('C:\\\\Users\\\\SANJAY\\\\SENTIMENT_ANALYSIS\\\\abc.txt','r')\n",
    "text=f.read()\n",
    "f.close()\n",
    "print(text)\n",
    "###############################\n",
    "text_t=tf.constant(text)\n",
    "text_t=tf.expand_dims(text_t,-1)\n",
    "print(text_t)\n",
    "#################\n",
    "pred=export_model.predict(text_t)\n",
    "print(pred)\n",
    "out=np.where(pred<0.5,'NEG','POS')\n",
    "print(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
